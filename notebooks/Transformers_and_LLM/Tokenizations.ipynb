{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "226d4003",
   "metadata": {},
   "source": [
    "# Tokenization: Evolution, Types, and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2b1d3e",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92fbb4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Tokenization is a fundamental process in natural language processing (NLP) that involves breaking down text into smaller units called tokens. These tokens can be words, subwords, characters, or other meaningful elements. Tokenization has evolved significantly over the years, adapting to the needs of various NLP tasks and models. This blog explores the history of tokenization, different types of tokenization, their pros and cons, and the compression ratio of num_bytes/num_tokens for each type.\n",
    "\n",
    "## History of Tokenization\n",
    "\n",
    "Tokenization began with simple word-based approaches, where text was split into individual words. As NLP models grew in complexity, more sophisticated methods were developed to handle nuances in language, such as subword tokenization and character-level tokenization.\n",
    "\n",
    "### Important metrics of Tokenization\n",
    "\n",
    "#### Compression Ratio Analysis\n",
    "The compression ratio of num_bytes/num_tokens is an important metric in tokenization. It indicates how efficiently text is represented in token form. A lower ratio is generally beneficial as it means fewer bytes are used per token, leading to more efficient storage and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7961065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, .\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))  # @inspect num_bytes\n",
    "    num_tokens = len(indices)                       # @inspect num_tokens\n",
    "    return num_bytes / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8269e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "class Tokenizer(ABC):\n",
    "    \"\"\"Abstract interface for a tokenizer.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7530e749",
   "metadata": {},
   "source": [
    "# Types of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb01b9",
   "metadata": {},
   "source": [
    "# Character Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242b9e2",
   "metadata": {},
   "source": [
    "\n",
    "Description: Character tokenization splits text into individual characters.\n",
    "\n",
    "Example Sentence: \"Tokenization is essential for NLP.\"\n",
    "\n",
    "Tokenized Output: [\"T\", \"o\", \"k\", \"e\", \"n\", \"i\", \"z\", \"a\", \"t\", \"i\", \"o\", \"n\", \" \", \"i\", \"s\", \" \", \"e\", \"s\", \"s\", \"e\", \"n\", \"t\", \"i\", \"a\", \"l\", \" \", \"f\", \"o\", \"r\", \" \", \"N\", \"L\", \"P\"] (tokens Corresponding to vocabulary set that can be assigned to corresponding indices)\n",
    "\n",
    "Number of Bytes: 34 (UTF-8 encoding) \n",
    "Number of Tokens: 34 \n",
    "\n",
    "$ \n",
    "\\text{Compression Ratio: } \\frac{34 \\text{ bytes}}{34 \\text{ tokens}} = 1.0\n",
    "$\n",
    "\n",
    "### Pros:\n",
    "    1. Simplest form of tokenization.\n",
    "    2. Handles any text without out-of-vocabulary issues.\n",
    "\n",
    "### Cons:\n",
    "    1. Inefficient for long texts.\n",
    "    2. Produces very large token sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1ddbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CharacterTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))\n",
    "    def raw_decode(self, indices: list[int])-> list[int]:\n",
    "        return list(map(chr, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54273bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterTokenizer()\n",
    "string = \"Tokenization is essential for NLP.\"  # @inspect string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41044e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 105, 115, 32, 101, 115, 115, 101, 110, 116, 105, 97, 108, 32, 102, 111, 114, 32, 78, 76, 80, 46]\n"
     ]
    }
   ],
   "source": [
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d2e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', ' ', 'f', 'o', 'r', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.raw_decode(indices)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd728b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is essential for NLP.\n"
     ]
    }
   ],
   "source": [
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "print(reconstructed_string)\n",
    "assert string == reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a31d0aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "compression_ratio = get_compression_ratio(string, indices)\n",
    "print(compression_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9300a1",
   "metadata": {},
   "source": [
    "# Byte-Based Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647f86c",
   "metadata": {},
   "source": [
    "Description: Byte-based tokenization treats each byte of the text as a token. This method is particularly useful for handling any kind of text data, including non-standard characters and binary data.\n",
    "\n",
    "Example Sentence: \"Tokenization is essential for NLP.\"\n",
    "\n",
    "Tokenized Output: [84, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 105, 115, 32, 101, 115, 115, 101, 110, 116, 105, 97, 108, 32, 102, 111, 114, 32, 78, 76, 80, 46]\n",
    "\n",
    "Number of Bytes: 34 (UTF-8 encoding)\n",
    "\n",
    "Number of Tokens: 34\n",
    "\n",
    "$ \n",
    "\\text{Compression Ratio: } \\frac{34 \\text{ bytes}}{34 \\text{ tokens}} = 1.0\n",
    "$\n",
    "\n",
    "### Pros:\n",
    "    Handles any text data, including non-standard characters.\n",
    "    No out-of-vocabulary issues.\n",
    "\n",
    "### Cons:\n",
    "\n",
    "    Produces very large token sequences.\n",
    "    Less interpretable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a26b678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ByteTokenizer(Tokenizer):\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")  # @inspect string_bytes\n",
    "        indices = list(map(int, string_bytes))  # @inspect indices\n",
    "        return indices\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)  # @inspect string_bytes\n",
    "        string = string_bytes.decode(\"utf-8\")  # @inspect string\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3849367",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteTokenizer()\n",
    "string = \"Tokenization is essential for NLP.\"  # @inspect string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf96a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 111, 107, 101, 110, 105, 122, 97, 116, 105, 111, 110, 32, 105, 115, 32, 101, 115, 115, 101, 110, 116, 105, 97, 108, 32, 102, 111, 114, 32, 78, 76, 80, 46]\n"
     ]
    }
   ],
   "source": [
    "indices = tokenizer.encode(string)  # @inspect indices\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f164f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is essential for NLP.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "reconstructed_string = tokenizer.decode(indices)  # @inspect reconstructed_string\n",
    "print(reconstructed_string)\n",
    "assert string == reconstructed_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb217bd3",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde4328a",
   "metadata": {},
   "source": [
    "\n",
    "Description: Word tokenization splits text into individual words based on spaces and punctuation.\n",
    "\n",
    "Example Sentence: \"Tokenization is essential for NLP.\"\n",
    "\n",
    "Tokenized Output: [\"Tokenization\", \"is\", \"essential\", \"for\", \"NLP\"] (tokens Corresponding to vocabulary set that can be assigned to corresponding indices)\n",
    "\n",
    "Number of Bytes: 34 (UTF-8 encoding) \n",
    "Number of Tokens: 5 \n",
    "\n",
    "$\n",
    "\\text{Compression Ratio: } \\frac{34 \\text{ bytes}}{5 \\text{ tokens}} = 6.8\n",
    "$\n",
    "\n",
    "### Pros: \n",
    "    1. Simple and intuitive.\n",
    "    2. Works well for languages with clear word boundaries.\n",
    "\n",
    "### Cons:\n",
    "    1. Struggles with out-of-vocabulary words.\n",
    "    2. Inefficient for morphologically rich languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d3ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19e96e13",
   "metadata": {},
   "source": [
    "# Subword Tokenization (e.g., Byte-Pair Encoding - BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502d53f",
   "metadata": {},
   "source": [
    "\n",
    "Description: Subword tokenization breaks words into smaller units, often based on frequency of subword pairs.\n",
    "\n",
    "Example Sentence: \"Tokenization is essential for NLP.\"\n",
    "\n",
    "Tokenized Output: [\"Token\", \"ization\", \"is\", \"essential\", \"for\", \"N\", \"L\", \"P\"] (tokens Corresponding to vocabulary set that can be assigned to corresponding indices)\n",
    "\n",
    "Number of Bytes: 34 (UTF-8 encoding) \n",
    "Number of Tokens: 8\n",
    "\n",
    "$ \n",
    "\\text{Compression Ratio: } \\frac{34 \\text{ bytes}}{8 \\text{ tokens}} = 4.25\n",
    "$\n",
    "\n",
    "### Pros:\n",
    "    1. Handles out-of-vocabulary words better.\n",
    "    2. Efficient for morphologically rich languages.\n",
    "\n",
    "### Cons:\n",
    "    1. More complex implementation.\n",
    "    2. May produce less interpretable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01d823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad1b0780",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a63664c",
   "metadata": {},
   "source": [
    "# SentencePiece Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10339b06",
   "metadata": {},
   "source": [
    "Description: SentencePiece is a subword tokenization method that treats text as a sequence of Unicode characters and uses a model to tokenize.\n",
    "\n",
    "Example Sentence: \"Tokenization is essential for NLP.\"\n",
    "\n",
    "Tokenized Output: [\"▁Token\", \"ization\", \"▁is\", \"▁essential\", \"▁for\", \"▁NLP\"] (tokens Corresponding to vocabulary set that can be assigned to corresponding indices)\n",
    "\n",
    "Number of Bytes: 34 (UTF-8 encoding) \n",
    "Number of Tokens: 6 \n",
    "\n",
    "$ \n",
    "\\text{Compression Ratio: } \\frac{34 \\text{ bytes}}{6 \\text{ tokens}} = 5.67\n",
    "$\n",
    "\n",
    "### Pros:\n",
    "    1. Language-agnostic.\n",
    "    2. Efficient for various languages and domains.\n",
    "### Cons:\n",
    "    1. Requires training a model.\n",
    "    2. May produce less interpretable tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce4bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1edec718",
   "metadata": {},
   "source": [
    "### Summary of Compression Ratios\n",
    "    Word Tokenization: 6.8\n",
    "    Subword Tokenization: 4.25\n",
    "    Character Tokenization: 1.0\n",
    "    SentencePiece Tokenization: 5.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f15f98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
